import os
from pathlib import Path
from openai import OpenAI
from dotenv import load_dotenv

# [ë³€ê²½] ChromaDB ë° LangChain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

# 1. í™˜ê²½ ì„¤ì •
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ì „ì—­ ë³€ìˆ˜ (ChromaDB VectorStore)
vectorstore = None

# ê²½ë¡œ ì„¤ì •
CURRENT_FILE_PATH = Path(__file__).resolve() 
PROJECT_ROOT = CURRENT_FILE_PATH.parent.parent 
PROMPT_PATH = PROJECT_ROOT / "utils" / "system_prompt.md" 

# [ë³€ê²½] ChromaDB ë°ì´í„° ê²½ë¡œ (../data/financial_terms)
CHROMA_DB_PATH = PROJECT_ROOT / "data" / "financial_terms"
COLLECTION_NAME = "financial_terms"

def load_knowledge_base():
    """ChromaDB ì—°ê²° ì„¤ì •"""
    global vectorstore
    if vectorstore is not None: return

    print("â³ [RAG] ChromaDB ì—°ê²° ì¤‘...")
    try:
        # ì„ë² ë”© ëª¨ë¸ ì„¤ì • (ì €ì¥í•  ë•Œ ì‚¬ìš©í•œ ëª¨ë¸ê³¼ ë™ì¼í•´ì•¼ í•¨)
        embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
        
        # ì €ì¥ëœ DB ë¡œë“œ
        vectorstore = Chroma(
            persist_directory=str(CHROMA_DB_PATH),
            embedding_function=embeddings,
            collection_name=COLLECTION_NAME,
            collection_metadata={"hnsw:space": "cosine"}
        )
        print(f"âœ… ChromaDB ì—°ê²° ì™„ë£Œ (ê²½ë¡œ: {CHROMA_DB_PATH})")
        
    except Exception as e:
        print(f"âŒ ChromaDB ì—°ê²° ì˜¤ë¥˜: {e}")
        vectorstore = None

def read_prompt_file():
    """MD íŒŒì¼ì—ì„œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì½ê¸°"""
    try:
        with open(PROMPT_PATH, "r", encoding="utf-8") as f:
            return f.read()
    except Exception:
        return "You are a helpful assistant." # íŒŒì¼ ì—†ì„ ì‹œ ê¸°ë³¸ê°’

# ğŸ”¥ í•µì‹¬ í•¨ìˆ˜: ChromaDB ê²€ìƒ‰ìœ¼ë¡œ ë³€ê²½
# finrag_agent.py ë‚´ë¶€

def get_rag_answer(korean_query, original_query=None):
    if vectorstore is None: load_knowledge_base()

    relevant_docs = []
    
    # 1. ë¬¸ì„œ ê²€ìƒ‰
    if vectorstore:
        results = vectorstore.similarity_search_with_score(korean_query, k=3)
        relevant_docs = results
    
    # ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
    if relevant_docs:
        print("ğŸ“‘ [Retrieved Docs]:")
        for doc, score in relevant_docs:
            # ê±°ë¦¬(Distance)ë¥¼ ìœ ì‚¬ë„(Similarity)ë¡œ ë³€í™˜í•˜ì—¬ ì¶œë ¥ (1 - distance)
            similarity = 1 - score
            print(f"   - {doc.metadata.get('word', 'Unknown')} (ìœ ì‚¬ë„: {similarity:.4f})")
    else:
        print("âš ï¸ [Retrieved Docs]: ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
    
    # 2. ì»¨í…ìŠ¤íŠ¸ ë° ì¶œì²˜(Citation) êµ¬ì„±
    context_text = ""
    citations = []
    
    if relevant_docs:
        for doc, score in relevant_docs:
            word = doc.metadata.get("word", "Term")
            raw_content = doc.page_content  # "ë”ë¸”ë”¥: ê²½ê¸°ì¹¨ì²´ê°€..." í˜•íƒœ
            
            # ğŸ› ï¸ [ìˆ˜ì • í¬ì¸íŠ¸] ë‚´ìš©ì—ì„œ "ë‹¨ì–´: " ë¶€ë¶„ ì œê±°í•˜ê¸°
            # ì €ì¥í•  ë•Œ "Word: Definition" í˜•ì‹ìœ¼ë¡œ ì €ì¥í–ˆìœ¼ë¯€ë¡œ, ì²« ë²ˆì§¸ ì½œë¡ (:) ë’¤ë§Œ ì”ë‹ˆë‹¤.
            if ":" in raw_content:
                definition = raw_content.split(":", 1)[1].strip()
            else:
                definition = raw_content
            
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context_text += f"Term: {word}\nDefinition: {definition}\n\n"
            
            # ì¶œì²˜ êµ¬ì„± (ìœ ì‚¬ë„ ê³„ì‚° í¬í•¨)
            similarity = 1 - score
            citations.append(f"- **{word}**: {definition[:50]}... (ìœ ì‚¬ë„: {similarity:.2f})")
    else:
        context_text = "ê´€ë ¨ëœ DB ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ì§€ì‹ì„ í™œìš©í•˜ì„¸ìš”."
        citations.append("- ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

    # 3. í”„ë¡¬í”„íŠ¸ ë¡œë”© ë° êµ¬ì„±
    system_template = read_prompt_file()
    formatted_system_prompt = system_template.format(context=context_text)

    # 4. LLM í˜¸ì¶œ
    response = client.chat.completions.create(
        model="gpt-5-mini",
        messages=[
            {"role": "system", "content": formatted_system_prompt},
            {"role": "user", "content": f"ì§ˆë¬¸ì— ëŒ€í•´ ì´ˆë“±í•™ìƒ ì„ ìƒë‹˜ì²˜ëŸ¼ í•µì‹¬ë§Œ ì§§ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”: {korean_query}"}
        ]
    )
    
    ai_answer = response.choices[0].message.content.strip()

    # 5. ìµœì¢… ì¶œë ¥ í¬ë§·íŒ…
    final_output = f"""
### ğŸŒ ì§ˆë¬¸ (Question)
- **Original**: {original_query if original_query else korean_query}
- **Translated**: {korean_query}

### ğŸ’¡ ì„ ìƒë‹˜ì˜ ë‹µë³€
{ai_answer}

---
### ğŸ“š ì°¸ê³  ë¬¸í—Œ (References)
{chr(10).join(citations)}
    """
    
    return final_output

if __name__ == "__main__":
    load_knowledge_base()
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    print(get_rag_answer("ì§‘ì„ êµ¬í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼í•´?", "How can I find a house?"))