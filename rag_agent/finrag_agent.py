import os
import time
from datetime import datetime
from pathlib import Path
from typing import TypedDict, Literal, Any
from dotenv import load_dotenv

# ë²¡í„° DB ë° LLM (LangChain í˜¸í™˜ ìœ ì§€)
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langgraph.graph import StateGraph, START, END

from rag_agent.web_search_rag import WebSearchRAG

# 1. í™˜ê²½ ì„¤ì •
load_dotenv()

# ê²½ë¡œ ì„¤ì •
CURRENT_FILE_PATH = Path(__file__).resolve()
PROJECT_ROOT = CURRENT_FILE_PATH.parent.parent
PROMPT_DIR = CURRENT_FILE_PATH.parent / "prompt" / "finrag"

CHROMA_DB_PATH = PROJECT_ROOT / "data" / "financial_terms"
COLLECTION_NAME = "financial_terms"

SIMILARITY_THRESHOLD = 0.6
WEB_SEARCH_KEYWORDS = ["í˜„ì¬", "ìµœì‹ ", "ì˜¤ëŠ˜", "ì£¼ê°€", "ì‹œì„¸", "ë‰´ìŠ¤", "ì „ë§", "ë‚ ì”¨", "ê²€ìƒ‰í•´ì¤˜", "ì–¼ë§ˆì•¼","ì§€ê¸ˆ","ê²€ìƒ‰","ê²€ìƒ‰í•´"]

# ì „ì—­ ë³€ìˆ˜
vectorstore = None
llm = ChatOpenAI(model="gpt-5-mini")
web_rag = WebSearchRAG() # ì›¹ ê²€ìƒ‰ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

def print_log(step_name: str, status: str, start_time: float = None, extra_info: str = None):
    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
    
    if status == "start":
        # flush=True ì¶”ê°€
        print(f"[{now}] â³ [{step_name}] ì‹œì‘...", flush=True) 
        return time.time()
        
    elif status == "end" and start_time is not None:
        elapsed = time.time() - start_time
        log_msg = f"[{now}] âœ… [{step_name}] ì™„ë£Œ (ì†Œìš”ì‹œê°„: {elapsed:.3f}ì´ˆ)"
        if extra_info:
            log_msg += f"\n   ğŸ‘‰ {extra_info}"
        
        # flush=True ì¶”ê°€
        print(log_msg, flush=True) 
        return elapsed

def load_prompt(filename: str) -> str:
    file_path = PROMPT_DIR / filename
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return f.read()
    except FileNotFoundError:
        now = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
        print(f"[{now}] âŒ [Error] í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return "{context}\n{question}"

def load_knowledge_base():
    """ChromaDB ì—°ê²° ì„¤ì •"""
    global vectorstore
    if vectorstore is not None:
        return
        
    t0 = print_log("RAG ChromaDB ì—°ê²°", "start")
    try:
        embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
        vectorstore = Chroma(
            persist_directory=str(CHROMA_DB_PATH),
            embedding_function=embeddings,
            collection_name=COLLECTION_NAME,
            collection_metadata={"hnsw:space": "l2"},
        )
        print_log("RAG ChromaDB ì—°ê²°", "end", t0, extra_info=f"Metric: L2, ê²½ë¡œ: {CHROMA_DB_PATH}")
    except Exception as e:
        now = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
        print(f"[{now}] âŒ ChromaDB ì—°ê²° ì˜¤ë¥˜: {e}")
        vectorstore = None

def format_web_result(web_result, original_query, translated_query):
    citations = [f"- **{src['title']}**: {src['url']}" for src in web_result.get("sources", [])]
    citation_text = "\n".join(citations) if citations else "- ì¶œì²˜ ì •ë³´ ì—†ìŒ"
    return f"""
### ğŸŒ ì§ˆë¬¸
- **Original**: {original_query if original_query else translated_query}
- **Translated**: {translated_query}

### ğŸŒ FinBotì˜ ì›¹ ê²€ìƒ‰ ë‹µë³€
{web_result['answer']}

---
### ğŸ“š ì°¸ê³  ì›¹ì‚¬ì´íŠ¸
{citation_text}
"""

# ---------------------------------------------------------
# [LangGraph] FinRAG ìƒíƒœ
# ---------------------------------------------------------
class FinRAGState(TypedDict, total=False):
    korean_query: str
    original_query: str
    use_web: bool
    relevant_docs: list
    context_text: str
    citations: list
    final_output: str

# ---------------------------------------------------------
# [LangGraph] ë…¸ë“œ
# ---------------------------------------------------------
def node_route(state: FinRAGState) -> dict:
    t0 = print_log("1. ê²€ìƒ‰ ë°©ì‹ ë¼ìš°íŒ… (node_route)", "start")
    korean_query = state["korean_query"]
    use_web = any(kw in korean_query for kw in WEB_SEARCH_KEYWORDS)
    
    extra = f"í‚¤ì›Œë“œ ê°ì§€ë¨ -> ì›¹ ê²€ìƒ‰ ì „í™˜" if use_web else "ì›¹ ê²€ìƒ‰ í‚¤ì›Œë“œ ì—†ìŒ -> ë‚´ë¶€ DB ê²€ìƒ‰"
    print_log("1. ê²€ìƒ‰ ë°©ì‹ ë¼ìš°íŒ… (node_route)", "end", t0, extra_info=extra)
    return {"use_web": use_web}

def node_web_search(state: FinRAGState) -> dict:
    t0 = print_log("2-A. ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ (node_web_search)", "start")
    korean_query = state["korean_query"]
    original_query = state.get("original_query")
    
    web_result = web_rag.web_search(korean_query)
    final_output = format_web_result(web_result, original_query, korean_query)
    
    print_log("2-A. ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ (node_web_search)", "end", t0, extra_info="ì›¹ ê²€ìƒ‰ ì™„ë£Œ ë° í¬ë§·íŒ…")
    return {"final_output": final_output}

def node_db_retrieve(state: FinRAGState) -> dict:
    t0 = print_log("2-B. ë²¡í„° DB ê²€ìƒ‰ (node_db_retrieve)", "start")
    global vectorstore
    if vectorstore is None:
        load_knowledge_base()
        
    korean_query = state["korean_query"]
    relevant_docs = []
    
    if vectorstore:
        try:
            results = vectorstore.similarity_search_with_score(korean_query, k=5)
            print(f"   ğŸ” [Search] '{korean_query}' DB ê²€ìƒ‰ ìˆ˜í–‰")
            for doc, score in results:
                if score <= SIMILARITY_THRESHOLD:
                    relevant_docs.append((doc, score))
                    print(f"      âœ… ì±„íƒ: {doc.metadata.get('word')} (ê±°ë¦¬: {score:.4f})")
                else:
                    print(f"      âŒ ì œì™¸: {doc.metadata.get('word')} (ê±°ë¦¬: {score:.4f} > {SIMILARITY_THRESHOLD})")
            relevant_docs = relevant_docs[:3]
        except Exception as e:
            now = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
            print(f"[{now}] âš ï¸ DB ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            
    print_log("2-B. ë²¡í„° DB ê²€ìƒ‰ (node_db_retrieve)", "end", t0, extra_info=f"ì¡°íšŒëœ ìœ íš¨ ë¬¸ì„œ ìˆ˜: {len(relevant_docs)}ê°œ")
    return {"relevant_docs": relevant_docs}

def node_web_fallback(state: FinRAGState) -> dict:
    t0 = print_log("3-A. ì›¹ ê²€ìƒ‰ìœ¼ë¡œ í´ë°± (node_web_fallback)", "start")
    extra = "ë‚´ë¶€ DBì— ê´€ë ¨ ì •ë³´ ì—†ìŒ (ìœ íš¨ ë¬¸ì„œ 0ê°œ) -> ì›¹ ê²€ìƒ‰ ìë™ ì „í™˜"
    print_log("3-A. ì›¹ ê²€ìƒ‰ìœ¼ë¡œ í´ë°± (node_web_fallback)", "end", t0, extra_info=extra)
    return node_web_search(state)

def node_db_answer(state: FinRAGState) -> dict:
    t0 = print_log("3-B. DB ê¸°ë°˜ ë‹µë³€ ìƒì„± (node_db_answer)", "start")
    korean_query = state["korean_query"]
    original_query = state.get("original_query")
    relevant_docs = state.get("relevant_docs") or []
    
    context_text = ""
    citations = []
    for doc, score in relevant_docs:
        word = doc.metadata.get("word", "Term")
        raw_content = doc.page_content
        definition = raw_content.split(":", 1)[1].strip() if ":" in raw_content else raw_content
        context_text += f"- **{word}**: {definition}\n"
        citations.append(f"- **{word}**: {definition[:60]}... (ê±°ë¦¬: {score:.4f})")

    system_template = load_prompt("finrag_01_system.md")
    rag_prompt = PromptTemplate.from_template(system_template)
    rag_chain = rag_prompt | llm | StrOutputParser()
    
    try:
        ai_answer = rag_chain.invoke({"context": context_text, "question": korean_query})
    except Exception as e:
        ai_answer = f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ({e})"

    final_output = f"""
### ğŸŒ ì§ˆë¬¸
- **Original**: {original_query if original_query else korean_query}
- **Translated**: {korean_query}

### ğŸ’¡ FinBotì˜ ë‹µë³€
{ai_answer}

---
### ğŸ“š ë‚´ë¶€ ì°¸ê³  ë¬¸í—Œ
{chr(10).join(citations)}
"""
    print_log("3-B. DB ê¸°ë°˜ ë‹µë³€ ìƒì„± (node_db_answer)", "end", t0)
    return {"final_output": final_output}

def route_after_start(state: FinRAGState) -> Literal["web_search", "db_retrieve"]:
    return "web_search" if state.get("use_web") else "db_retrieve"

def route_after_db(state: FinRAGState) -> Literal["web_fallback", "db_answer"]:
    return "web_fallback" if not (state.get("relevant_docs")) else "db_answer"

# ---------------------------------------------------------
# ê·¸ë˜í”„ ë¹Œë“œ
# ---------------------------------------------------------
_finrag_graph = None

def _get_finrag_graph():
    global _finrag_graph
    if _finrag_graph is None:
        builder = StateGraph(FinRAGState)
        builder.add_node("route", node_route)
        builder.add_node("web_search", node_web_search)
        builder.add_node("db_retrieve", node_db_retrieve)
        builder.add_node("web_fallback", node_web_fallback)
        builder.add_node("db_answer", node_db_answer)

        builder.add_edge(START, "route")
        builder.add_conditional_edges("route", route_after_start, {"web_search": "web_search", "db_retrieve": "db_retrieve"})
        builder.add_edge("web_search", END)
        builder.add_conditional_edges("db_retrieve", route_after_db, {"web_fallback": "web_fallback", "db_answer": "db_answer"})
        builder.add_edge("web_fallback", END)
        builder.add_edge("db_answer", END)
        _finrag_graph = builder.compile()
    return _finrag_graph

def get_rag_answer(korean_query, original_query=None):
    print("\n" + "-"*50)
    total_t0 = print_log("FinRAG ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸", "start")
    
    if vectorstore is None:
        load_knowledge_base()
        
    graph = _get_finrag_graph()
    initial: FinRAGState = {"korean_query": korean_query, "original_query": original_query}
    result = graph.invoke(initial)
    
    print("-"*50)
    print_log("FinRAG ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸", "end", total_t0)
    print("-"*50 + "\n")
    
    return result.get("final_output", "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    load_knowledge_base()
    print(get_rag_answer("ê¸ˆë¦¬ê°€ ë­ì•¼?"))
    print("=" * 60)
    print(get_rag_answer("í˜„ì¬ ì‚¼ì„±ì „ì ì£¼ê°€ ì•Œë ¤ì¤˜"))
    print("=" * 60)
    print(get_rag_answer("ì•„ì´ìœ  ìµœì‹  ì•¨ë²” ë­ì•¼?"))